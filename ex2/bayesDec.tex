\newif\ifvimbug
\vimbugfalse

\ifvimbug
\begin{document}
\fi

\exercise{Bayesian Decision Theory}
In this exercise, we consider data produced by a mixture of two Gaussian distributions with parameters $\{\mu_1$, $\sigma_1\}$ and $\{\mu_2$, $\sigma_2\}$. Each Gaussian represents a class labeled $C_1$ and $C_2$, respectively. 

\begin{questions}

%----------------------------------------------

\begin{question}{Optimal Boundary}{4}
Explain in one short sentence what Bayesian Decision Theory is. What is its goal? 
What condition does hold at the optimal decision boundary? When do we decide for class $C_1$ over $C_2$?

\begin{answer}
	Bayesian Decision Theory models needs an explicit distribution of features. It's goal is to minimize the error probability. If x is a feature vector, than the corresponding sample is classifies as member of $C_1$ if $p(C_1 | x) > p(C_2|x)$ and vice versa. Therefore, the optimal decision boundary $x_{optimal}$ fulfills the equation $p(C_1 | x_{optimal}) = p(C_2 | c_{optimal})$

\end{answer}

\end{question}


%----------------------------------------------

\begin{question}{Decision Boundaries}{8}
If both classes have equal prior probabilities $p(C_1) = p(C_2)$ and the same variance $\sigma_1 = \sigma_2$, derive the decision boundary $x^*$ analytically as a function of the two means $\mu_1$ and $\mu_2$.

\begin{answer}
	Gaussian distribution: $\frac{1}{\sqrt{2 \pi \sigma^2 }2} exp(- \frac{x-\mu}{2 \sigma ^2})$
	
	Since prior probabilities are equal, the optimal decision boundary is at the point where the distributions are equal.
	\begin{align*}
		\frac{1}{\sqrt{2 \pi \sigma^2 }} exp(- \frac{(x-\mu_1)^2}{2 \sigma ^2})
		&= 		\frac{1}{\sqrt{2 \pi \sigma^2 }} exp(- \frac{(x-\mu_2)^2}{2 \sigma ^2})\\
		\frac{(x-\mu_1)^2}{2 \sigma ^2}
		&= \frac{(x-\mu_2)^2}{2 \sigma ^2}\\
		-2x\mu_1 + \mu_1^2 &= -2x\mu_2 + \mu_2^2\\
		x &= \frac{\mu_2^2 - \mu_1^2}{2(\mu_2 - \mu_1)}\\
		x &= \frac{\mu_2 + \mu_1}{2}
	\end{align*}
	
	The symmetry of the Gaussian distribution could be used as well to derive this value.
\end{answer}

\end{question}

%----------------------------------------------

\begin{question}{Different Miss-Classification Costs}{8}
Assume $\mu_1 > 0$, $\mu_1 = 2\, \mu_2$, and $\sigma_1=\sigma_2$. If misclassifying sample $x \in C_2$ as class $C_1$ is four times more expensive than the opposite, how does the decision boundary change? Derive the boundary analytically.
(There is no cost for correctly classifying samples.)

\begin{answer}

\begin{align*}
R_1 &= R_2\\
4 * p(C_2|x) & = p(C_1|x)\\
4 * exp(- \frac{(x-\mu_2)^2}{2 \sigma ^2}) &= exp(- \frac{(x-\mu_1)^2}{2 \sigma ^2})\\
- \frac{(x-\mu_2)^2}{2 \sigma ^2} + \frac{(x-\mu_1)^2}{2 \sigma ^2} &= -\ln{4}\\
2x\mu_2 - \mu_2^2 - 2x\mu_1 + \mu_1^2 &= -2\sigma^2 \ln{4}\\
x &=\frac{-2\sigma^2 \ln{4}}{2(\mu_2 - \mu_1)} + \frac{\mu_2^2 - \mu_1^2}{2(\mu_2 - \mu_1)}\\
x &=\frac{-\sigma^2 \ln{4}}{\mu_2 - \mu_1} + \frac{\mu_2 + \mu_1}{2}\\
x &=\frac{-\sigma^2 \ln{4}}{\mu} + \frac{3\mu}{2}\\
\end{align*}

\end{answer}

\end{question}

%----------------------------------------------

\end{questions}
